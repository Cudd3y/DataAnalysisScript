---
title: "Data Analysis Script of the Pointing Experiment Project 2"
output: html_document
---

```{r setup, message=FALSE, echo=FALSE}

## Clear workspace
rm(list=ls())

# Load Packages
pacman::p_load(
  # data processing
  plyr,
  tidyverse,  # collection of the tidyverse packages
  stringr,    #   - for string functions
  forcats,    #   - utility functions for working with factor levels
  lubridate,  #   - utility for parsing and performing arithematic on dates 
  tools,      #   - for package development, administration and documentation
  data.table, #   - for data manipulation 
  tibble,      #   - to create data frames
  broom      #   - for tidy()
)

import::from(car, qqPlot, ncvTest)
import::from(nortest, lillie.test)
import::from(lawstat, levene.test)
```

### Data Import
Load all data that is stored in the corresponding folder
```{r import}
# Create a function that imports all .csv files from a directory "ExperimentalData"
# Put all your data files from the data collection software in the folder "ExperimentalData".
# file_list <- list.files(path = file.path("ExperimentalData"), pattern = "*.csv", recursive = TRUE, full.names = TRUE)

# data_raw <- do.call("rbind", lapply(file_list, function(filename)
  # data.frame(read.delim(filename, sep = ";"))))

# because of a coding problem, participants 11 - 17 were coded as 1 - 7. To correct this problem, we import the data for each participant separately. 
data1 <- data.frame(read.delim("ExperimentalData/pointing_1.csv", sep = ";"))
data2 <- data.frame(read.delim("ExperimentalData/pointing_2.csv", sep = ";"))
data3 <- data.frame(read.delim("ExperimentalData/pointing_3.csv", sep = ";"))
data4 <- data.frame(read.delim("ExperimentalData/pointing_4.csv", sep = ";"))
data5 <- data.frame(read.delim("ExperimentalData/pointing_5.csv", sep = ";"))
data6 <- data.frame(read.delim("ExperimentalData/pointing_6.csv", sep = ";"))
data7 <- data.frame(read.delim("ExperimentalData/pointing_7.csv", sep = ";"))
data8 <- data.frame(read.delim("ExperimentalData/pointing_8.csv", sep = ";"))
data9 <- data.frame(read.delim("ExperimentalData/pointing_9.csv", sep = ";"))
data10 <- data.frame(read.delim("ExperimentalData/pointing_10.csv", sep = ";"))
data11 <- data.frame(read.delim("ExperimentalData/pointing_11.csv", sep = ";"))
data11$Part <- rep(11,nrow(data11))
data12 <- data.frame(read.delim("ExperimentalData/pointing_12.csv", sep = ";"))
data12$Part <- rep(12,nrow(data12))
data13 <- data.frame(read.delim("ExperimentalData/pointing_13.csv", sep = ";"))
data13$Part <- rep(13,nrow(data13))
data14 <- data.frame(read.delim("ExperimentalData/pointing_14.csv", sep = ";"))
data14$Part <- rep(14,nrow(data14))
data15 <- data.frame(read.delim("ExperimentalData/pointing_15.csv", sep = ";"))
data15$Part <- rep(15,nrow(data15))
data16 <- data.frame(read.delim("ExperimentalData/pointing_16.csv", sep = ";"))
data16$Part <- rep(16,nrow(data16))
data17 <- data.frame(read.delim("ExperimentalData/pointing_17.csv", sep = ";"))
data17$Part <- rep(17,nrow(data17))
data_raw <- rbind(data1, data2, data3, data4, data5, data6, data7, data8, data9, data10, data11, data12, data13, data14, data15, data16, data17)

rm(data1, data2, data3, data4, data5, data6, data7, data8, data9, data10, data11, data12, data13, data14, data15, data16, data17)
```

### Data Clean Up
Before we start with the analysis, we will reduce the dataset to the necessary attributes and calcutate the aggregated values.
```{r clean up}
# Keep only the columns of interest and give them more readable names
data <- data_raw %>% 
  select(
    Participant = Part,
    Condition = Cond,
    TrialNumber = Rep,
    Block,
    TargetNumber = Click,
    TargetIsHit = Succ,
    TargetHighlighted = Tar_t,
    TargetClicked = Click_t,
    PosX,
    PosY,
    Distance = Dist
  )

# Remove all Demo Trial from the data set
data <- data %>%
  filter(TrialNumber != "demo")

data$Participant <- as.factor(data$Participant)
data$Block <- as.factor(data$Block)

# Renaming of cell values in a more meaningful way
data$Condition[data$Condition == '0'] <- "Regular Edge"
data$Condition[data$Condition == '1'] <- "Virtual Edge"
data$Condition <- as.factor(data$Condition)

options(digits.secs = 3) # Set accuracy to one millisecond
# Convert the timestamps into a time format
data$TargetHighlighted <- as.POSIXct(data$TargetHighlighted, format = "%H:%M:%OS")
data$TargetClicked <- as.POSIXct(data$TargetClicked, format = "%H:%M:%OS")
```

### Compute movment time and convert errors to a boolean
```{r}
data <- data %>%
  mutate(MovementTime = as.numeric(TargetClicked - TargetHighlighted, unit ="secs"))

data$TargetIsHit[data$TargetIsHit == '0'] <- FALSE
data$TargetIsHit[data$TargetIsHit == '1'] <- TRUE
```

### Calculate values per trial
```{r}
data_per_trial <- data %>%
  group_by(Participant, Condition, TrialNumber)  %>%
  summarise(ErrorRate = 1 - mean(TargetIsHit, na.rm = T), MovementTime = sum(MovementTime, na.rm = T))

# Data for trials in Regular Edge condition
data_per_trial_regular_edge <- data_per_trial %>%
  filter(Condition == "Regular Edge")

# Data for trials in Virtual Edge condition
data_per_trial_virtual_edge <- data_per_trial %>%
  filter(Condition == "Virtual Edge")
```


## Exploratory plot
*Note:* With "trial" we mean the completion of one pointing exercise containing 10 sequences (blocks) and every sequence is composed of 5 targets on which the participant has to click.

### Average movement time per participant for regular edge condition (lineplot)
Lineplot which displays the average time that each participant needed to complete a trial.
```{r lineplot_for_movement_time}
ggplot (data = data_per_trial_regular_edge, mapping = aes(x = TrialNumber, y = MovementTime, col = Participant, group = Participant)) +
  geom_point() +
  geom_line() +
  labs(x = 'Trial Number - Regular Edge', y = 'Movement Time per Trial in sec')
```
```{r lineplot_for_error_rate_plot_average}
# Lineplot of the calculated error rate average
ggplot (data = data_per_trial_regular_edge, mapping = aes(x = TrialNumber, y = ErrorRate, col = Participant, group = Participant)) +
  geom_point() +
  geom_line() +
  labs(x = 'Trial Number - Regular Edge', y = 'Error Rate in %')
```

*Note:* With "trial" we mean the completion of one pointing exercise containing 10 sequences and every sequence is composed of 6 targets on which the participant has to click.

### Average movement time per participant for virtual edge condition (lineplot)
Lineplot which displays the average time that each participant needed to complete a task.
```{r lineplot_for_movement_time_task}
ggplot (data = data_per_trial_virtual_edge, mapping = aes(x = TrialNumber, y = MovementTime, col = Participant, group = Participant)) +
  geom_point() +
  geom_line() +
  labs(x = 'Trial Number - Virtual Edge', y = 'Movement Time per Trial in sec')
```

### Average error rate per participant for virtual edge condition (lineplot)
```{r lineplot_for_error_rate_plot}
# Lineplot of the calculated error rate average
ggplot (data = data_per_trial_virtual_edge, mapping = aes(x = TrialNumber, y = ErrorRate, col = Participant, group = Participant)) +
  geom_point() +
  geom_line() +
  labs(x = 'Trial Number - Virtual Edge', y = 'Error Rate in %')
```


### Relationship between MovementTime and Regular Edge Trials.
```{r}
data_per_trial_regular_edge %>% 
  ggplot(., aes(x = TrialNumber, y = MovementTime)) +
  geom_point() +
  geom_smooth(method = "lm", size = 1.5, color = "red")

```
### Relationship between MovementTime and Virtual Edge Trials.
```{r}
data_per_trial_virtual_edge %>% 
  ggplot(., aes(x = TrialNumber, y = MovementTime)) +
  geom_point() +
  geom_smooth(method = "lm", size = 1.5, color = "red")
```


### Movemement time: mean and unadjusted confidence interval for each trial and each condition.
```{r}
ggplot(data_per_trial, aes(TrialNumber, MovementTime, color=Condition)) +
  stat_summary(fun.data = "mean_cl_normal", position = position_dodge(width = 1)) + scale_color_brewer(palette = "Set1")
```

```{r}
ggplot(data_per_trial, aes(TrialNumber, MovementTime, fill=Condition)) +
  stat_summary(fun.data = "mean_cl_normal", geom = "bar", position = "dodge") +  scale_color_brewer(palette = "Set1")
```
Movement time: we see that the mean for virtual edges is slightly smaller and the confidence intervals overlap. We also see the difference between the two conditions seem to increase with the number of trials.

### Relationship between Error Rate and Regular Edge Trials.
```{r}
data_per_trial_regular_edge %>% 
  ggplot(., aes(x = TrialNumber, y = ErrorRate)) +
  geom_point() +
  geom_smooth(method = "lm", size = 1.5, color = "red")

```
### Relationship between Error Rate and Virtual Edge Trials.
```{r}
data_per_trial_virtual_edge %>% 
  ggplot(., aes(x = TrialNumber, y = ErrorRate)) +
  geom_point() +
  geom_smooth(method = "lm", size = 1.5, color = "red")
```


### Error Rate: mean and unadjusted confidence interval for each trial and each condition.
```{r}
ggplot(data_per_trial, aes(TrialNumber, ErrorRate, color=Condition)) +
  stat_summary(fun.data = "mean_cl_normal", position = position_dodge(width = 1)) + scale_color_brewer(palette = "Set1")
```

```{r}
ggplot(data_per_trial, aes(TrialNumber, ErrorRate, fill=Condition)) +
  stat_summary(fun.data = "mean_cl_normal", geom = "bar", position = "dodge") +  scale_color_brewer(palette = "Set1")
```
Error Rate: We see that the mean for virtual edges is smaller and the confidence intervals do not lap.

## Assumbption on the data (original data)
### Normality assumption for Movement Time
```{r}
par(mfrow = c(1,2)) #plot two plots next to each other
data_per_trial %>%
  group_by(Condition) %>%
  do({
    qqPlot(.$MovementTime)
    title(main=.$Condition[[1]])
    tidy(lillie.test(.$MovementTime))
  })
par(mfrow = c(1,1)) # sets back the default
```
## Homogeneity Assumption for Movement Time
```{r message=FALSE, warning=FALSE, include=FALSE}
levene.test(data_per_trial$MovementTime, data_per_trial$Condition, location = "median")
```

### Normality assumption for Error Rate
```{r}
par(mfrow = c(1,2)) #plot two plots next to each other
data_per_trial %>%
  group_by(Condition) %>%
  do({
    qqPlot(.$ErrorRate)
    title(main=.$Condition[[1]])
    tidy(lillie.test(.$ErrorRate))
  })
par(mfrow = c(1,1)) # sets back the default
```

## Homogeneity Assumption for Error Rate
```{r message=FALSE, warning=FALSE, include=FALSE}
levene.test(data_per_trial$ErrorRate, data_per_trial$Condition, location = "median")
```

## Log Transformation for Error Rate
### Normality assumption for log of Error Rate
```{r}
par(mfrow = c(1,2)) #plot two plots next to each other
data_per_trial %>%
  group_by(Condition) %>%
  do({
    qqPlot(log(.$ErrorRate + 2))
    title(main=.$Condition[[1]])
    tidy(lillie.test(log(.$ErrorRate + 2)))
  })
par(mfrow = c(1,1)) # sets back the default
```

## Homogeneity Assumption for Error Rate
```{r message=FALSE, warning=FALSE, include=FALSE}
levene.test(data_per_trial$ErrorRate, data_per_trial$Condition, location = "median")
```

## Statistical analysis
### Linear model for MovementTime
We fit a linear model between MovementTime, Condition and their interaction.
```{r}
fit_movementTime <- lm(MovementTime ~ Condition + TrialNumber + Condition*TrialNumber, data_per_trial)
summary(fit_movementTime)
```
Then, we plot the residuals to check if they seem to be normally distributed
```{r}
p_M_C_resid <- data_per_trial %>% 
  ggplot(., aes(x = MovementTime, y = fit_movementTime$residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, col = "red")
p_M_C_resid
```

```{r}
car::qqPlot(fit_movementTime$residuals)
```
From QQ-plot, we see that the residuals are normally distributed.

### Analysis of variance (ANOVA)
Comparing to the null model: (there's no relationship between movementTime and condition)
```{r}
anova_r <- anova(fit_movementTime)
anova_t <- tidy(anova_r)   # extract parameters for the report
anova_t
```

### AIC model assessment
Goal: find the model with lowest AIC
```{r}
step_backward_result <- 
  MASS::stepAIC(fit_movementTime, 
    direction = "backward", 
    trace = TRUE
    )
```

**Interpretation:** Comparing between the two models: 

* Null model (\\(H_0\\)): there will be no significant prediction of movement time by condition and trial number.
* Alternative model (\\(H_1\\)): the predictors condition and trial number adequately represents the data.

Since the p-values for both condition and trial number are large, we cannot reject our null hypothesis.

Accordingly, the AIC-score shows the best (lowest) value when the trial number is removed.


### Linear model for Error Rate
We fit a linear model between Error Rate, Condition and their interaction.
```{r}
fit_errorRate <- lm(log(ErrorRate + 2) ~ Condition + TrialNumber + Condition*TrialNumber, data_per_trial)
summary(fit_errorRate)
```
Then, we plot the residuals to check if they seem to be normally distributed
```{r}
p_E_C_resid <- data_per_trial %>% 
  ggplot(., aes(x = log(ErrorRate + 2), y = fit_errorRate$residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, col = "red")
p_E_C_resid
```

```{r}
car::qqPlot(fit_errorRate$residuals)
```
From QQ-plot, we see that the residuals do not follow a normal distribution.

### Analysis of variance (ANOVA)
```{r}
anova_r <- anova(fit_errorRate)
anova_t <- tidy(anova_r)   # extract parameters for the report
anova_t
```

### AIC model assessment
Goal: find the model with lowest AIC
```{r}
step_backward_result <- 
  MASS::stepAIC(fit_errorRate, 
    direction = "backward", 
    trace = TRUE
    )
```

**Interpretation:** Comparing between the two models: 

* Null model (\\(H_0\\)): there will be no significant prediction of error rate by condition and trial number.
* Alternative model (\\(H_1\\)): the predictors condition and trial number adequately represents the data.

From the anova results, we can see that the null hypothesis can be rejected. Nevertheless, only one predictor is significant (condition), while trial number is not.

However from the AIC, we see that the best model is the one including both predictors. The difference with the model removing trial number is very small, though.
