---
title: "Data Analysis Script of the Pointing Experiment Project 2"
output: html_document
---

```{r setup, message=FALSE, echo=FALSE}

## Clear workspace
rm(list=ls())

# =============================================
# Load Packages
pacman::p_load(
  # data processing
  plyr,
  tidyverse,  # collection of the tidyverse packages
  stringr,    #   - for string functions
  forcats,    #   - utility functions for working with factor levels
  lubridate,  #   - utility for parsing and performing arithematic on dates 
  tools,      #   - for package development, administration and documentation
  data.table, #   - for data manipulation 
  tibble,      #   - to create data frames
  broom       #   - for tidy()
)
```

## Data Import
Load all Data that is stored in the corresponding folder
```{r import}
# Create a function that imports all .csv files from a directory "ExperimentalData"
# TODO: Put the gathered data in the "ExperimentalData" Folder & use the second line below
file_list <- list.files(path = file.path("PilotData"), pattern = "*.csv", recursive = TRUE, full.names = TRUE)
# file_list <- list.files(path = file.path("ExperimentalData"), pattern = "*.csv", recursive = TRUE, full.names = TRUE)

data_raw <- do.call("rbind", lapply(file_list, function(filename)
  data.frame(read.delim(filename, sep = ";"))))

# Keep only the columns of interest and give them more readable names
data <- data_raw %>% 
  select(
    Participant = Part,
    Condition = Cond,
    Block,
    TrialNumber = Trial,
    TargetNumber = Click,
    TargetHighlighted = Click_t,
    TargetClicked = Tar_t,
    PosX,
    PosY,
    Distance = Dist,
    TargetIsHit = Succ
  )

```

## Data Clean Up
Before we start with the analysis, we will reduce the dataset to the necessary attributes and calcutate the aggregated values.
```{r clean up}
# Keep only the columns of interest and give them more readable names
data <- data_raw %>% 
  select(
    Participant = Part,
    Condition = Cond,
    Block,
    TrialNumber = Trial,
    TargetNumber = Click,
    TargetHighlighted = Click_t,
    TargetClicked = Tar_t,
    PosX,
    PosY,
    Distance = Dist,
    TargetIsHit = Succ
  )


# Renaming of cell values in a more meaningful way
data$Condition[data$Condition == '0'] <- "Regular Edge"
data$Condition[data$Condition == '1'] <- "Virtual Edge"
# TODO if the Demo-Block is still included in the data
data$Block[data$Block == '0'] <- "Demo"
data$TargetIsHit[data$TargetIsHit == '0'] <- FALSE
data$TargetIsHit[data$TargetIsHit == '1'] <- TRUE
# TODO Controll if dataformat still matches the one from the files imported
data$TargetHighlighted <- as.POSIXct(data$TargetHighlighted, format = "%d-%m-%Y %H:%M:%S:%OS")
data$TargetClicked <- as.POSIXct(data$TargetClicked, format = "%d-%m-%Y %H:%M:%S:%OS")

# Option to display the ms as well. -> Doesn't work properly yet... Should work with the new Timestamp Format
options(digits.secs = 3)

# TODO Delet this part if the Demo-Block is no longer included in the data
# Remove all Demo Trial from the data set
data <- data %>%
  filter(Block != "Demo")

# =============================================
# CALCULATE VALUES PER SEQUENCE
# Calculate Error_Rate for each sequence
temp_get_TargetsHit <- data %>%
  group_by(Condition, Participant, TrialNumber, Block)  %>%
  summarise(ErrorRate = 1 - mean(TargetIsHit))


# Calculate Movement Time for each sequence
# Get first Timestamp (TargetHighlighted)of  a sequence
temp_get_start_time <- data %>%
  select(
    Participant,
    Condition,
    Block,
    TrialNumber,
    TargetNumber,
    StartTime = TargetHighlighted) %>%
  filter(TargetNumber == 1) %>%
  select(-TargetNumber)

# Get last Timestamp (TargetClicked) of a sequence
temp_get_end_time <- data %>%
  select(
    Participant,
    Condition,
    Block,
    TrialNumber,
    TargetNumber,
    EndTime = TargetClicked) %>%
  filter(TargetNumber == 5) %>%
  select(-TargetNumber)

# Create table with both first and last timestamp of a sequence and calculate the difference
data_per_sequence <- data %>%
  select(
    Participant,
    Condition,
    Block,
    TrialNumber
    ) %>%
  full_join(temp_get_start_time) %>%
  full_join(temp_get_end_time) %>%
  mutate(MovementTime = as.numeric(EndTime - StartTime, unit ="secs")) %>%
  unique() %>%
  full_join(temp_get_TargetsHit)

# =============================================
# CALCULATE VALUES FOR WHOLE TRIAL
# Calculate total (sum) movement time per trial 
movement_time_per_trial <- aggregate(MovementTime ~ Condition + Participant + TrialNumber, data = data_per_sequence, sum)

# Calculate average (mean) error rate per trial
error_rate_per_trial <- aggregate(ErrorRate ~ Condition + Participant + TrialNumber, data = data_per_sequence, mean)

# Data for each trial
data_per_trial <- movement_time_per_trial %>%
  full_join(error_rate_per_trial)

# Data for trials in Regular Edge condition
data_regular_edge_per_trial <- data_per_trial %>%
  filter(Condition == "Regular Edge")

# Data for trials in Virtual Edge condition
data_virtual_edge_per_trial <- data_per_trial %>%
  filter(Condition == "Virtual Edge")

```

## Plots for regular edge condition
*Note:* With "trial" we mean the completion of one pointing exercise containing 10 sequences and every sequence is composed of 6 targets on which the participant has to click.

### Average movement time per participant for regular edge condition (lineplot)
Lineplot which displays the average time that each participant needed to complete a trial.
```{r lineplot_for_movement_time}
ggplot (data = data_regular_edge_per_trial, mapping = aes(x = TrialNumber, y = MovementTime, col = Participant, group = Participant)) +
  geom_point() +
  geom_line() +
  labs(x = 'Trial Number - Regular Edge', y = 'Movement Time per Trial in sec')
```

```{r lineplot_for_error_rate_plot}
# Lineplot of the calculated error rate average
ggplot (data = data_regular_edge_per_trial, mapping = aes(x = TrialNumber, y = ErrorRate, col = Participant, group = Participant)) +
  geom_point() +
  geom_line() +
  labs(x = 'Trial Number - Regular Edge', y = 'Error Rate in %')
```

## Plots for virtual edge condition
*Note:* With "trial" we mean the completion of one pointing exercise containing 10 sequences and every sequence is composed of 6 targets on which the participant has to click.

### Average movement time per participant for virtual edge condition (lineplot)
Lineplot which displays the average time that each participant needed to complete a task.
```{r lineplot_for_movement_time}
ggplot (data = data_virtual_edge_per_trial, mapping = aes(x = TrialNumber, y = MovementTime, col = Participant, group = Participant)) +
  geom_point() +
  geom_line() +
  labs(x = 'Trial Number - Virtual Edge', y = 'Movement Time per Trial in sec')
```

```{r lineplot_for_error_rate_plot}
# Lineplot of the calculated error rate average
ggplot (data = data_virtual_edge_per_trial, mapping = aes(x = TrialNumber, y = ErrorRate, col = Participant, group = Participant)) +
  geom_point() +
  geom_line() +
  labs(x = 'Trial Number - Virtual Edge', y = 'Error Rate in %')
```


# Exploratory plot
Plotting the relationship between in MovementTime and Regular Edge Trials.
```{r}
p_time <- 
  data_regular_edge_per_trial %>% 
  ggplot(., aes(x = TrialNumber, y = MovementTime)) +
  geom_point() +
  geom_smooth(method = "lm", size = 1.5, color = "red")
p_time
```
Plotting the relationship between in MovementTime and Virtual Edge Trials.
```{r}
p_time1 <- 
  data_virtual_edge_per_trial %>% 
  ggplot(., aes(x = TrialNumber, y = MovementTime)) +
  geom_point() +
  geom_smooth(method = "lm", size = 1.5, color = "red")
p_time1
```

----

#### Statistical analysis

### Linear model for movementTime
We fit a linear model between MovementTime and condition.
```{r}
lm_movementTime_condition <- lm(MovementTime ~ Condition , data_per_trial)
summary(lm_movementTime_condition)
```

Then, we plot the residuals to check if they seem to be normally distributed
```{r}
p_M_C_resid <- data_per_trial %>% 
  ggplot(., aes(x = MovementTime, y = lm_movementTime_condition$residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, col = "red")
p_M_C_resid
```

```{r}
car::qqPlot(lm_movementTime_condition$residuals)
```
From QQ-plot, the residuals is not really normally distributed.

# Analysis of variance (ANOVA)
Comparing to the null model: (there's no relationship between movementTime and condition)
```{r}
anova_r <- anova(lm_movementTime_condition)
anova_r
anova_t <- tidy(anova_r)   # extract parameters for the report
```

# AIC model assessment
Goals: find the model with lowest AIC
```{r}
wholeModel <- lm(MovementTime ~ Condition + TrialNumber, data = data_per_trial)
step_backward_result <- 
  MASS::stepAIC(wholeModel, 
    direction = "backward", 
    trace = TRUE
    )
```

**Interpretation:** Comparing between the two models: 

* Null model (\\(H_0\\)): the grand mean adequately represents the data
* Alternative model (\\(H_1\\)): three means adequately represents the data

Assuming that \\((H_0\\)) is true, the data doesn't quite compatible with \\(H_0\\), *F*(`r anova_t$df[1]`, `r anova_t$df[2]`) = `r anova_t$statistic[1]`, *p* = `r anova_t$p.value[1]`. Therefore, we choose to beleive in \\(H_1\\).


We also know the location of each mean from the coefficients (calculated by `summary(m1)`. However, the SE of each coefficient differs from the SE of each group, because some errors were accounted for by the SE of the intercept. Therefore, we are still uncertain about the difference between each pair of groups.

----

### Linear model for Error Rate
We fit a linear model between Error Rate and condition.
```{r}
lm_errorRate_condition <- lm(ErrorRate ~ Condition , data_per_trial)
summary(lm_errorRate_condition)
```

Then, we plot the residuals to check if they seem to be normally distributed
```{r}
p_E_C_resid <- data_per_trial %>% 
  ggplot(., aes(x = ErrorRate, y = lm_errorRate_condition$residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, col = "red")
p_E_C_resid
```

```{r}
car::qqPlot(lm_errorRate_condition$residuals)
```
From QQ-plot, the residuals is not really normally distributed.

# Analysis of variance (ANOVA)
Comparing to the null model: (there's no relationship between movementTime and condition)
```{r}
anova_r <- anova(lm_errorRate_condition)
anova_r
anova_t <- tidy(anova_r)   # extract parameters for the report
```

# AIC model assessment
Goals: find the model with lowest AIC
```{r}
wholeModel <- lm(ErrorRate ~ Condition + TrialNumber, data = data_per_trial)
step_backward_result <- 
  MASS::stepAIC(wholeModel, 
    direction = "backward", 
    trace = TRUE
    )
```

**Interpretation:** Comparing between the two models: 

* Null model (\\(H_0\\)): the grand mean adequately represents the data
* Alternative model (\\(H_1\\)): three means adequately represents the data

Assuming that \\((H_0\\)) is true, the data doesn't quite compatible with \\(H_0\\), *F*(`r anova_t$df[1]`, `r anova_t$df[2]`) = `r anova_t$statistic[1]`, *p* = `r anova_t$p.value[1]`. Therefore, we choose to beleive in \\(H_1\\).


We also know the location of each mean from the coefficients (calculated by `summary(m1)`. However, the SE of each coefficient differs from the SE of each group, because some errors were accounted for by the SE of the intercept. Therefore, we are still uncertain about the difference between each pair of groups.


----

### Code vom Project 1
### ANOVA MovementTime & ErroRate
```{r anova_speed, echo=FALSE}
#Speed / Movement Time
fit_speed <- aov(MovementTime ~ Condition, data_per_trial)
summary(fit_speed)
m.res <- resid(fit_speed)
plot(data_per_trial$MovementTime, m.res, ylab="Residuals", xlab="MovementTime")
```

```{r anova_accuracy, echo=FALSE}
#Accuracy / Error Rate
fit_accuracy <- aov(ErrorRate ~ Condition, data_per_trial)
summary(fit_accuracy)
m.res <- resid(fit_accuracy)
plot(data_per_trial$ErrorRate, m.res, ylab="Residuals", xlab="ErrorRate")
```

### Tukey Test

#### Speed
Summary of the Tukey test for Speed:
```{r tukeyhsd_speed, echo=FALSE}
TukeyHSD(fit_speed)
par(mar=c(5,10,4,2)+0.1) # Add additional margin to the left for fitting the label size

# This hack is used to set a more specific title than the default one generated by R
par(col.main = "white")
plot(TukeyHSD(fit_speed), las=1)
par(col.main = "black")
title(main = "Speed - 95% family-wise confidence level")
```


#### Accuracy
Summary of the Tukey test for Accuracy:
```{r tukeyhsd_accuracy, echo=FALSE}
TukeyHSD(fit_accuracy)
par(mar=c(5,10,4,2)+0.1) # Add additional margin to the left for fitting the label size

# This hack is used to set a more specific title than the default one generated by R
par(col.main = "white")
plot(TukeyHSD(fit_accuracy), las=1)
par(col.main = "black")
title(main = "Accuracy - 95% family-wise confidence level")
```

