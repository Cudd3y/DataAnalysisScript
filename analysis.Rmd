---
title: "Data Analysis Script of the Pointing Experiment Project 2"
output: html_document
---

```{r setup, message=FALSE, echo=FALSE}

## Clear workspace
rm(list=ls())

# =============================================
# Load Packages
pacman::p_load(
  # data processing
  plyr,
  tidyverse,  # collection of the tidyverse packages
  stringr,    #   - for string functions
  forcats,    #   - utility functions for working with factor levels
  lubridate,  #   - utility for parsing and performing arithematic on dates 
  tools,      #   - for package development, administration and documentation
  data.table, #   - for data manipulation 
  tibble,      #   - to create data frames
  broom      #   - for tidy()
)
```

## Data Import
Load all Data that is stored in the corresponding folder
```{r import}
# Create a function that imports all .csv files from a directory "ExperimentalData"
# TODO: Put the gathered data in the "ExperimentalData" Folder & use the second line below
file_list <- list.files(path = file.path("PilotData"), pattern = "*.csv", recursive = TRUE, full.names = TRUE)
# file_list <- list.files(path = file.path("ExperimentalData"), pattern = "*.csv", recursive = TRUE, full.names = TRUE)

data_raw <- do.call("rbind", lapply(file_list, function(filename)
  data.frame(read.delim(filename, sep = ";"))))

```

## Data Clean Up
Before we start with the analysis, we will reduce the dataset to the necessary attributes and calcutate the aggregated values.
```{r clean up}
# Keep only the columns of interest and give them more readable names
data <- data_raw %>% 
  select(
    Participant = Part,
    Condition = Cond,
    TrialNumber = Trial,
    Block,
    TargetNumber = Click,
    TargetIsHit = Succ,
    TargetHighlighted = Click_t,
    TargetClicked = Tar_t,
    PosX,
    PosY,
    Distance = Dist
  )

# TODO Delete this part if the Demo-Block is no longer included in the data
# Remove all Demo Trial from the data set
data <- data %>%
  filter(Block != "0")

data$Participant <- as.factor(data$Participant)

# Renaming of cell values in a more meaningful way
data$Condition[data$Condition == '0'] <- "Regular Edge"
data$Condition[data$Condition == '1'] <- "Virtual Edge"
data$Condition <- as.factor(data$Condition)

#data$TrialNumber <- as.factor(data$TrialNumber)
#data$Block <- as.factor(data$Block)

options(digits.secs = 3)
# TODO Control if dataformat still matches the one from the files imported
data$TargetHighlighted <- as.POSIXct(data$TargetHighlighted, format = "%d-%m-%Y %H:%M:%OS3")
data$TargetClicked <- as.POSIXct(data$TargetClicked, format = "%d-%m-%Y %H:%M:%OS3")




```

```{r error management}
# ERROR MANAGEMENT
dat_error <- data[data$Block_start > data$Block_end, ]

# if the target click lays before the target highlight, we look which of the target click value or the target highlight value is odd and remove this one. By odd, we mean that the difference with the next value in the same column is negative (for both the lagged and lead value).
data$TargetClicked[data$TargetClicked < data$TargetHighlighted & (data$TargetClicked - lag(data$TargetClicked) < 0 | lead(data$TargetClicked) - data$TargetClicked < 0)] <- NA

data$TargetHighlighted[data$TargetClicked < data$TargetHighlighted & (lead(data$TargetHighlighted) - data$TargetHighlighted < 0 | data$TargetHighlighted - lag(data$TargetHighlighted) < 0)] <- NA


# Add Columns with the start time and the end time of a block
data <- data %>%
  mutate(Block_start = case_when(TargetNumber == 1 ~ TargetHighlighted, TargetNumber == 2 ~ lag(TargetHighlighted, 1), TargetNumber == 3 ~ lag(TargetHighlighted, 2), TargetNumber == 4 ~ lag(TargetHighlighted, 3), TargetNumber == 5 ~ lag(TargetHighlighted, 4))) %>%
  mutate(Block_end = case_when(TargetNumber == 1 ~ lead(TargetClicked, 4), TargetNumber == 2 ~ lead(TargetClicked, 3), TargetNumber == 3 ~ lead(TargetClicked, 2), TargetNumber == 4 ~ case_when(lead(TargetNumber) == 5 ~ lead(TargetClicked, 1), lead(TargetNumber) != 5 ~ TargetClicked), TargetNumber == 5 ~ TargetClicked)) %>%
  select (Participant, Condition, TrialNumber, Block, TargetNumber, Block_start, Block_end, TargetIsHit, TargetHighlighted, TargetClicked, PosX, PosY, Distance)


# if the block end lays after the block start, we set block end and target click time to NA (should not be the case after the previous steps)
data$Block_end[data$Block_start > data$Block_end] <- NA
data$TargetClicked[data$Block_start > data$Block_end] <- NA

# if the time of the target highlight or the target click lays outside of the block time window, the value is turned to NA
data$TargetHighlighted[data$TargetHighlighted < data$Block_start | data$TargetHighlighted > data$Block_end] <- NA
data$TargetClicked[data$TargetClicked < data$Block_start | data$TargetClicked > data$Block_end] <- NA
data$TargetClicked[data$TargetClicked - data$TargetHighlighted > 20] <- NA

```

```{r}
# COMPUTE MOVEMENT TIME AND SHOW ERRORS

data <- data %>%
  mutate(MovementTime = as.numeric(TargetClicked - TargetHighlighted, unit ="secs"))

data$TargetIsHit[data$TargetIsHit == '0'] <- FALSE
data$TargetIsHit[data$TargetIsHit == '1'] <- TRUE

```


```{r}
# =============================================
# CALCULATE VALUES PER BLOCK
data_per_block <- data %>%
  group_by(Participant, Condition, TrialNumber, Block)  %>%
  summarise(ErrorRate = 1 - mean(TargetIsHit, na.rm = T), MovementTime = sum(MovementTime, na.rm = T)) 

```

```{r}
# =============================================
# CALCULATE VALUES PER TRIAL
data_per_trial <- data %>%
  group_by(Participant, Condition, TrialNumber)  %>%
  summarise(ErrorRate = 1 - mean(TargetIsHit, na.rm = T), MovementTime = sum(MovementTime, na.rm = T))


# Data for trials in Regular Edge condition
data_per_trial_regular_edge <- data_per_trial %>%
  filter(Condition == "Regular Edge")

# Data for trials in Virtual Edge condition
data_per_trial_virtual_edge <- data_per_trial %>%
  filter(Condition == "Virtual Edge")

```


## Plots for regular edge condition
*Note:* With "trial" we mean the completion of one pointing exercise containing 10 sequences/blocks and every sequence is composed of 6 targets on which the participant has to click.

### Average movement time per participant for regular edge condition (lineplot)
Lineplot which displays the average time that each participant needed to complete a trial.
```{r lineplot_for_movement_time}
ggplot (data = data_per_trial_regular_edge, mapping = aes(x = TrialNumber, y = MovementTime, col = Participant, group = Participant)) +
  geom_point() +
  geom_line() +
  labs(x = 'Trial Number - Regular Edge', y = 'Movement Time per Trial in sec')
```

```{r lineplot_for_error_rate_plot}
# Lineplot of the calculated error rate average
ggplot (data = data_per_trial_regular_edge, mapping = aes(x = TrialNumber, y = ErrorRate, col = Participant, group = Participant)) +
  geom_point() +
  geom_line() +
  labs(x = 'Trial Number - Regular Edge', y = 'Error Rate in %')
```

## Plots for virtual edge condition
*Note:* With "trial" we mean the completion of one pointing exercise containing 10 sequences and every sequence is composed of 6 targets on which the participant has to click.

### Average movement time per participant for virtual edge condition (lineplot)
Lineplot which displays the average time that each participant needed to complete a task.
```{r lineplot_for_movement_time}
ggplot (data = data_per_trial_virtual_edge, mapping = aes(x = TrialNumber, y = MovementTime, col = Participant, group = Participant)) +
  geom_point() +
  geom_line() +
  labs(x = 'Trial Number - Virtual Edge', y = 'Movement Time per Trial in sec')
```

```{r lineplot_for_error_rate_plot}
# Lineplot of the calculated error rate average
ggplot (data = data_per_trial_virtual_edge, mapping = aes(x = TrialNumber, y = ErrorRate, col = Participant, group = Participant)) +
  geom_point() +
  geom_line() +
  labs(x = 'Trial Number - Virtual Edge', y = 'Error Rate in %')
```


# Exploratory plot
Plotting the relationship between in MovementTime and Regular Edge Trials.
```{r}
p_time <- 
  data_per_trial_regular_edge %>% 
  ggplot(., aes(x = TrialNumber, y = MovementTime)) +
  geom_point() +
  geom_smooth(method = "lm", size = 1.5, color = "red")
p_time
```
Plotting the relationship between in MovementTime and Virtual Edge Trials.
```{r}
p_time1 <- 
  data_per_trial_virtual_edge %>% 
  ggplot(., aes(x = TrialNumber, y = MovementTime)) +
  geom_point() +
  geom_smooth(method = "lm", size = 1.5, color = "red")
p_time1
```

----

#### Statistical analysis

### Linear model for movementTime
We fit a linear model between MovementTime and condition.
```{r}
fit_movementTime <- lm(MovementTime ~ Condition + TrialNumber, data_per_trial)
summary(fit_movementTime)
```

Then, we plot the residuals to check if they seem to be normally distributed
```{r}
p_M_C_resid <- data_per_trial %>% 
  ggplot(., aes(x = MovementTime, y = fit_movementTime$residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, col = "red")
p_M_C_resid
```

```{r}
car::qqPlot(fit_movementTime$residuals)
```
From QQ-plot, the residuals are not normally distributed but their distribution is quite close.

# Analysis of variance (ANOVA)
Comparing to the null model: (there's no relationship between movementTime and condition)
```{r}
anova_r <- anova(fit_movementTime)
anova_r
anova_t <- tidy(anova_r)   # extract parameters for the report
```

# AIC model assessment
Goal: find the model with lowest AIC
```{r}
step_backward_result <- 
  MASS::stepAIC(fit_movementTime, 
    direction = "backward", 
    trace = TRUE
    )
```

**Interpretation:** Comparing between the two models: 

* Null model (\\(H_0\\)): there will be no significant prediction of movement time by condition and trial number.
* Alternative model (\\(H_1\\)): the predictors condition and trial number adequately represents the data.

Since the p-values for both condition and trial number are large, we cannot reject our null hypothesis.

Accordingly, the AIC-score shows the best (lowest) value when both condition and trial number are removed.

----

### Linear model for Error Rate
We fit a linear model between Error Rate and condition.
```{r}
fit_errorRate <- lm(ErrorRate ~ Condition + TrialNumber, data_per_trial)
summary(fit_errorRate)
```

Then, we plot the residuals to check if they seem to be normally distributed
```{r}
p_E_C_resid <- data_per_trial %>% 
  ggplot(., aes(x = ErrorRate, y = fit_errorRate$residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, col = "red")
p_E_C_resid
```

```{r}
car::qqPlot(fit_errorRate$residuals)
```
From QQ-plot, we see that the residuals follow a normal distribution.

# Analysis of variance (ANOVA)
```{r}
anova_r <- anova(fit_errorRate)
anova_r
anova_t <- tidy(anova_r)   # extract parameters for the report
```

# AIC model assessment
Goal: find the model with lowest AIC
```{r}
step_backward_result <- 
  MASS::stepAIC(fit_errorRate, 
    direction = "backward", 
    trace = TRUE
    )
```

**Interpretation:** Comparing between the two models: 

* Null model (\\(H_0\\)): there will be no significant prediction of error rate by condition and trial number.
* Alternative model (\\(H_1\\)): the predictors condition and trial number adequately represents the data.

From the anova results, we can see that the null hypothesis can be rejected. Nevertheless, only one predictor is significant (condition), while trial number is not.

However from the AIC, we see that the best model is the one including both predictors. The difference with the model removing trial number is very small, though.





